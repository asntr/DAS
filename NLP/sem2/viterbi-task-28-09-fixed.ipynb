{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание на семинар 28 сентября по курсу анализа текстов ШАД"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тема: Определение частей речи с помощью скрытой марковской модели и алгоритма Витерби. \n",
    "\n",
    "\n",
    "\n",
    "**Дедлайн**:   9:00 утра 5 октября (для заочников 9.00 утра 8 октября)\n",
    "\n",
    "**Среда выполнения**: Jupyter Notebook (Python 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "В данной  работе вам предстоит:\n",
    "\n",
    "- обучить скрытую марковскую модель на размеченных данных\n",
    "- реализовать алгоритм Витерби для декодирования\n",
    "\n",
    "Задание сдается в контест."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение частей речи (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем решать задачу определения частей речи (POS-теггинга) с помощью скрытой марковской модели (HMM). Формула совместной плотности наблюдаемых и скрытых переменных задается как\n",
    "\n",
    "$$ p(X, T) = p(T) p(X|T) = p(t_1)  \\prod_{i=2}^N p(t_i|t_{i-1}) \\prod_{i=1}^N p(x_i|t_i)$$\n",
    "\n",
    "В данном случае:\n",
    "\n",
    "- наблюдаемые переменные $X$ - это слова корпуса;\n",
    "\n",
    "- скрытые переменные $T$ - это POS-теги."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Обучение HMM на размеченных данных\n",
    "\n",
    "Требуется построить скрытую марковскую модель и настроить все ее параметры с помощью оценок максимального правдоподобия по размеченным данным (последовательности пар слово+тег):\n",
    "\n",
    "- Вероятности переходов между скрытыми состояниями $p(t_i | t_{i - 1})$ посчитайте на основе частот биграмм POS-тегов.\n",
    "\n",
    "- Вероятности эмиссий наблюдаемых состояний $p(x_i | t_i)$ посчитайте на основе частот \"POS-тег - слово\".\n",
    "\n",
    "- Обратите внимание на проблему разреженности счетчиков и сделаейте все вероятности сглаженными по Лапласу (add-one smoothing).\n",
    "\n",
    "- Распределение вероятностей начальных состояний $p(t_1)$ задайте равномерным.\n",
    "\n",
    "Обратите внимание, что так как мы используем размеченные данные, то у нас нет необходимости в оценивании апостериорных вероятностей на скрытые переменные с помощью алгоритма forward-backword и использовании EM-алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите brown корпус с универсальной системой тегирования. Для этого вам понадобятся ресурсы brown и universal_tagset из nltk.download().  В этой системе содержатся следующие теги:\n",
    "\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition\t(on, of, at, ...)\n",
    "- ADV - adverb\t(really, already, still, ...)\n",
    "- CONJ\t- conjunction\t(and, or, but, ...)\n",
    "- DET - determiner, article\t(the, a, some, ...)\n",
    "- NOUN\t- noun\t(year, home, costs, ...)\n",
    "- NUM - numeral\t(twenty-four, fourth, 1991, ...)\n",
    "- PRT -\tparticle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- .\t- punctuation marks\t(. , ;)\n",
    "- X\t- other\t(ersatz, esprit, dunno, ...)\n",
    "\n",
    "Тегсеты в корпусах текстов и в различных теггерах могут быть разными. Но есть маппер: http://www.nltk.org/_modules/nltk/tag/mapping.html\n",
    "\n",
    "Проанализируйте данные, с которыми Вы работаете. В частности, ответьте на вопросы:\n",
    "- Каков общий объем датасета, формат?\n",
    "- Приведены ли слова к нижнему регистру? Чем  это нам может в дальнейшем помешать?\n",
    "- Как распределены слова в корпусе?  Как распределены теги в корпусе?\n",
    "\n",
    "Сделайте случайное разбиение выборки на обучение и контроль в отношении 9:1 и обучите скрытую марковскую модель из предыдущего пункта. Если впоследствии обучение моделей будет занимать слишком много времени, работайте с подвыборкой, например, только текстами определенных категорий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(tagset=\"universal\")\n",
    "\n",
    "\n",
    "def train_hmm(tagged_sents):\n",
    "    \"\"\"\n",
    "    Calucaltes p(tag), p(word|tag), p(tag|tag) from corpus.\n",
    "\n",
    "    Args:\n",
    "        tagged_sents: list of list of tagged tokens. \n",
    "            Example: \n",
    "            [[('dog', 'NOUN'), ('eats', 'VERB'), ...], ...]\n",
    "\n",
    "    Returns:\n",
    "        p_t, p_w_t, p_t_t - tuple of 3 elements:\n",
    "        p_t - dict(float), tag -> proba\n",
    "        p_w_t - dict(dict(float), tag -> word -> proba\n",
    "        p_t_t - dict(dict(float), previous_tag -> tag -> proba\n",
    "    \"\"\"\n",
    "    alpha=1e-24\n",
    "    counter_tag = Counter()\n",
    "    counter_tag_tag = Counter()\n",
    "    counter_tag_word = Counter()\n",
    "    tags = set()\n",
    "    words = set()\n",
    "    p_t_t = defaultdict(lambda: defaultdict(float))\n",
    "    p_w_t = defaultdict(lambda: defaultdict(float))\n",
    "    p_t = dict()\n",
    "    \n",
    "    # your code here\n",
    "    for sent in tagged_sents:\n",
    "        sent = [(word.lower(), tag) for word, tag in sent]\n",
    "        sent_tags = [pair[1] for pair in sent]\n",
    "        sent_words = [pair[0] for pair in sent]\n",
    "        counter_tag.update(sent_tags)\n",
    "        tags.update(sent_tags)\n",
    "        counter_tag_tag.update([(ltag, rtag) for (_, ltag), (_, rtag) in zip(sent[:-1], sent[1:])])\n",
    "        counter_tag_word.update(sent)\n",
    "        words.update(sent_words)\n",
    "    for (ltag, rtag), count in counter_tag_tag.items():\n",
    "        p_t_t[ltag][rtag] = (count + alpha) / (counter_tag[ltag] + alpha * len(tags))\n",
    "    for (word, tag), count in counter_tag_word.items():\n",
    "        p_w_t[tag][word] = (count + alpha) / (counter_tag[tag] + alpha * (len(tags) + len(words)))\n",
    "    for tag in tags:\n",
    "        p_t[tag] = 1 / len(tags)\n",
    "    return p_t, p_w_t, p_t_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51606, 5734)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(brown_tagged_sents, test_size=0.1)\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Алгоритм Витерби для применения модели\n",
    "\n",
    "Чтобы использовать обученную модель для определения частей речи на новых данных, необходимо реализовать алгоритм Витерби. Это алгоритм динамиеского программирования, с помощью которого мы будем находить наиболее вероятную последовательность скрытых состояний модели для фиксированной последовательности слов:\n",
    "\n",
    "$$ \\hat{T} = \\arg \\max_{T} p(T|X) = \\arg \\max_{T} p(X, T) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, определяющую максимальную вероятность последовательности, заканчивающейся на $i$-ой позиции в состоянии $k$:\n",
    "\n",
    "$$\\delta(k, i) = \\max_{t_1, \\dots t_{i-1}} p(x_1, \\dots x_i, t_1, \\dots t_i=k)$$\n",
    "\n",
    "Тогда $\\max_{k} \\delta(k, N)$ - максимальная вероятность всей последовательности. А состояния, на которых эта вероятность достигается - ответ задачи.\n",
    "\n",
    "Алгоритм Витерби заключается в последовательном пересчете функции $\\delta(k, i)$ по формуле:\n",
    "\n",
    "$$\\delta(k, i) = \\max_{m} \\delta(m, i-1) p(t_i = k|t_{i-1} = m) p(x_i|t_i=k) $$\n",
    "\n",
    "Аналогично пересчитывается функция, определяющая, на каком состоянии этот максимум достигается:\n",
    "\n",
    "$$s(k, i) = \\arg \\max_{m} \\delta(m, i-1) p(t_i = k|t_{i-1} = m) p(x_i|t_i=k) $$\n",
    "\n",
    "\n",
    "На практике это означает заполнение двумерных массивов размерности: (длина последовательности) $\\times$ (количество возможных состояний). Когда массивы заполнены, $\\arg \\max_{k} \\delta(k, N)$ говорит о последнем состоянии. Начиная с него можно восстановить все состояния по массиву $s$. \n",
    "\n",
    "Осталось уточнить, как стартовать последовательный пересчет (чем заполнить первый столбец массива вероятностей):\n",
    "\n",
    "$$\\delta(k, 1) = p(k) p(x_1|t_1=k)$$\n",
    "\n",
    "В реализации рекомендуется перейти к логарифмам, т.к. произведение большого числа маленьких вероятностей может приводить к вычислительным ошибкам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_t, p_w_t, p_t_t = train_hmm(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi_algorithm(test_tokens_list, p_t, p_w_t, p_t_t):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        test_tokens_list: list of tokens. \n",
    "            Example: \n",
    "            ['I', 'go']\n",
    "        p_t: dict(float), tag->proba\n",
    "        p_w_t: - dict(dict(float), tag -> word -> proba\n",
    "        p_t_t: - dict(dict(float), previous_tag -> tag -> proba\n",
    "\n",
    "    Returns:\n",
    "        list of hidden tags\n",
    "    \"\"\"\n",
    "    delta = np.zeros([len(p_t), len(test_tokens_list)])\n",
    "    s = np.zeros([len(p_t), len(test_tokens_list)], dtype='int64')\n",
    "    states = list(p_t.keys())\n",
    "    delta[:, 0] = np.array([np.log(p_t[state] or 10 ** -300) + np.log(p_w_t[state][test_tokens_list[0]] or 10 ** -300)\n",
    "                            for state in states])\n",
    "    for i in range(1, len(test_tokens_list)):\n",
    "        for j in range(len(states)):\n",
    "            values = [\n",
    "                delta[k, i-1] + np.log(p_t_t[states[k]][states[j]] or 10 ** -300) + np.log(p_w_t[states[j]][test_tokens_list[i]] or 10 ** -300)\n",
    "                for k in range(len(states))\n",
    "            ]\n",
    "            delta[j, i] = np.max(values)\n",
    "            s[j, i] = np.argmax(values)\n",
    "    result = [np.argmax(delta[:, -1])]\n",
    "    for i in range(1, len(test_tokens_list)):\n",
    "        result.append(s[result[-1], -i])\n",
    "    return [states[i] for i in reversed(result)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте работу реализованного алгоритма на следующих модельных примерах, проинтерпретируйте результат.\n",
    "\n",
    "- 'he can stay'\n",
    "- 'a milk can'\n",
    "- 'i saw a dog'\n",
    "- 'an old saw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN', 'VERB', 'VERB', 'ADV', 'ADJ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_algorithm('language processing is extremely funny'.split(' '), p_t, p_w_t, p_t_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примените модель к отложенной выборке Брауновского корпуса и подсчитайте точность определения тегов (accuracy). Сделайте выводы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9018798124489181\n"
     ]
    }
   ],
   "source": [
    "correct_answers = 0\n",
    "total = 0\n",
    "for sent in test:\n",
    "    words = [el[0] for el in sent]\n",
    "    tags = [el[1] for el in sent]\n",
    "    predicted_tags = viterbi_algorithm(words, p_t, p_w_t, p_t_t)\n",
    "    for i in range(len(sent)):\n",
    "        if tags[i] == predicted_tags[i]:\n",
    "            correct_answers += 1\n",
    "        total += 1\n",
    "print('accuracy = {}'.format(correct_answers / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
